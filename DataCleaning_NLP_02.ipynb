{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WITHIN THIS: Basic cleaning is concucted and visualised, further key bigrams/trigrams were extracted and visualised."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## General data processing and visualisation use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import glob\n",
    "import plotly.express as px\n",
    "\n",
    "## For webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "## Machine learning / Deep learning classification models\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "## XGBoost as extra\n",
    "import xgboost as xgb\n",
    "\n",
    "## Set the pandas display option set to max_columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "## Natural language processing\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Remember that the script needs to be run at least 2x as too large of a range in finding observations at nwac.ac fails to error code 500** -> Will need to concat the data together to get one complete dataframe.\n",
    "\n",
    "**IN MY CASE:** They have been saved as **\"avalanche_occurance_1.csv\"** and **\"avalanche_occurance_2.csv\"**\n",
    "\n",
    "**MORE IN SECOND FILE**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base Data Cleaning (01)\n",
    "\n",
    "**data should be split into two dataframe (gained from running the script twice at two different ranges for dates.), will need to connect them together.**\n",
    "\n",
    "1. Null values are handeled\n",
    "\n",
    "Not much cleaning was needed to be done in terms for visualisation, as such only null values were the ones affected\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Get the elevation if exists out of location '''\n",
    "def location_to_elevation(entry):\n",
    "    entry = re.findall(r'\\b\\d+\\b', entry)\n",
    "    return entry"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Less recent dates\n",
    "df1 = pd.read_csv('avalanche_occurance_1.csv')\n",
    "# More recent dates\n",
    "df2 = pd.read_csv('avalanche_occurance_2.csv')\n",
    "\n",
    "# Concat both the datasets\n",
    "df = pd.concat([df2, df1])\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.to_csv('main_avalanche_observations_dataset.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the csv of avalanche observations and add to variable df\n",
    "df = pd.read_csv('main_avalanche_observations_dataset.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Given info_observation, fill in missing values with advanced_observations\n",
    "# In turn giving one column, drop the advanced_observations col\n",
    "df['info_observation'].fillna(df['advanced_observations'], inplace=True)\n",
    "df.drop(columns=['advanced_observations'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change nulls in [avalanche_Y/N, instability] to No\n",
    "df['avalanche_Y/N'].fillna('No', inplace=True)\n",
    "df['instability'].fillna('No', inplace=True)\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Given info_observation, fill in missing values with advanced_observations\n",
    "# In turn giving one column, drop the advanced_observations col\n",
    "df['info_observation'].fillna(df['advanced_observations'], inplace=True)\n",
    "df.drop(columns=['advanced_observations'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change nulls in [avalanche_Y/N, instability] to No\n",
    "df['avalanche_Y/N'].fillna('No', inplace=True)\n",
    "df['instability'].fillna('No', inplace=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get all the possible elevations given a location\n",
    "# - some have been added but not many -> thus not included.\n",
    "df['elevation'] = df['location_ele'].apply(location_to_elevation)\n",
    "df['elevation'].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dont think this specifically can be used as too many missing, however could be found with the use of NLP?\n",
    "df.elevation.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA - Base data cleaning (02)\n",
    "\n",
    "**Bit kind of rushed, need to refactor the code slightly**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Function to calculate the amoung of NULLs in a dataframe per its given columns\n",
    "--- INPUT: pandas dataframe\n",
    "--- OUTPUT: pandas dataframe\n",
    "'''\n",
    "\n",
    "def calc_null(dataframe):\n",
    "\n",
    "    # Initialise a new df\n",
    "    missing_df = pd.DataFrame(df.isna().sum(), columns=['missing_vals'])\n",
    "    missing_df['percentage'] = 0.0\n",
    "\n",
    "    # Initialise math and loop var\n",
    "    total_count = len(dataframe)\n",
    "\n",
    "    # Assign percentage\n",
    "    missing_df['percentage'] = round(missing_df['missing_vals'] / total_count * 100, 3)\n",
    "\n",
    "    # Sort accordingly\n",
    "    missing_df = missing_df.sort_values('percentage', ascending=False)\n",
    "\n",
    "    return missing_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_14352\\1710592408.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Avalanche_Y/N will not have any nulls as been web scraped.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mdisp_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'region'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'avalanche_Y/N'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue_counts\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mdisp_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdisp_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m \u001B[0mdisp_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"count\"\u001B[0m \u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'count'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mascending\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "''' Shows count of the avalanche occures categorised into Y/N'''\n",
    "\n",
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df.groupby('region')[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" }).sort_values(['count'], ascending=False)\n",
    "\n",
    "# Initialise figure\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.barplot(\n",
    "    data=disp_df,\n",
    "    x='region',\n",
    "    y='count',\n",
    "    hue='avalanche_Y/N'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment = 'right',\n",
    "    fontweight = 'light',\n",
    "    fontsize = 'large'\n",
    ")\n",
    "plt.title('Y/N occurances of avalanches per region')\n",
    "# plt.savefig('Y/N_avalanches_region.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Shows the missing percentage df '''\n",
    "\n",
    "# Use previously created function to check amount of missing values and %\n",
    "df_missing_original = calc_null(df)\n",
    "\n",
    "# Reset the index\n",
    "df_missing_original_disp = df_missing_original.reset_index()\n",
    "\n",
    "# Initialise the figure and run seaborn for visualisations.\n",
    "plt.figure(figsize = (6, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data = df_missing_original_disp,\n",
    "    x = 'index',\n",
    "    y = 'percentage'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment='right',\n",
    "    fontweight ='light',\n",
    "    fontsize ='large'\n",
    ")\n",
    "plt.title('percentage of nulls per column')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Unique display of avalanches per given feature '''\n",
    "\n",
    "u_df = df.describe().loc['unique'].copy()\n",
    "temp_df = pd.DataFrame({'unique_count':u_df})\n",
    "temp_df = temp_df.sort_values('unique_count', ascending=False)\n",
    "values = temp_df.unique_count\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.barplot(\n",
    "    data=temp_df,\n",
    "    x=temp_df.index,\n",
    "    y='unique_count'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment = 'right',\n",
    "    fontweight = 'light',\n",
    "    fontsize = 'large'\n",
    ")\n",
    "plt.title('unique counts per column')\n",
    "# plt.savefig('Y/N_avalanches_region.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check describe\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check info\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLP break down and testing for enrichment of dataset (03)\n",
    "\n",
    "**FURTHER IMPORVEMENTS FOR THIS SECTION IS TO SPLIT THE YES AND NO AVALANCHES AND THEN DO EXACLY WHAT I DID BELOW, EASIER TO FIND OUT WHAT KIND OF TEXTURE PER GIVEN INSTANCE OF AVALANCHE**\n",
    "\n",
    "**Further to figure out whether its past tense or not, find most common adjectives and go from there? - obviously need to do it for yes and no as the snow texture can be thought of as leading up**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_text(text, token=False):\n",
    "    ''' Cleans and preprocesses the input text '''\n",
    "\n",
    "    # Stop words test\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # stop_words = set()\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removal of:\n",
    "    punc = '!\"#$%&()*+, -./:;<=>?@[\\]^_`{|}~”“\\''\n",
    "    punc = [x for x in punc]\n",
    "    text = re.sub(r'http\\S+', '', text) # Urls\n",
    "    text = re.sub(r'<.*?>', '', text) #Html tags\n",
    "    text = re.sub(r'(@.+?)\\s', '', text)\n",
    "    text = re.sub(r'(//t.co/.+?)\\s', '', text)\n",
    "    text = re.sub(r'(//t.co/.+?)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # Special characters\n",
    "\n",
    "    # Tokenize text into a sentance\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens.extend(word_tokenize(sentence))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "    stop_words.update(['who', 'what', 'where', 'when', 'why', 'how', 'which'])\n",
    "    stop_words.update(['rt', '#', 'fav', '', ':', '@', '!', ';', '…','...', '(', ')', '~'])\n",
    "    stop_words.update(punc)\n",
    "\n",
    "    stop_words.remove('no')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Tokens into a string\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    if token:\n",
    "        # Tokenize\n",
    "        clean_text = word_tokenize(clean_text)\n",
    "\n",
    "    return clean_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check to see how well it works.\n",
    "string_original = df['info_observation'].head(1).values\n",
    "string = string_original[0]\n",
    "string_stem_test = clean_text(string)\n",
    "\n",
    "print(string_stem_test)\n",
    "print(\"\\n\")\n",
    "print(string_original)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# apply to the dataset as works\n",
    "df['observation_cleaned'] = df['info_observation'].apply(clean_text)\n",
    "df['observation_cleaned_tokens'] = df['observation_cleaned'].apply(word_tokenize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bigrams/Trigrams extraction & evaluation (04)\n",
    "\n",
    "**EXPLINATION:**\n",
    "1. **extract_ngrams_k():** function where k is the number of grams, will extract pairings of words. here ive done 2 to 4. Inputs it as a new column. called first to get the pairings.\n",
    "2. **collect_dict_bi_tri_quad()** function is called. Checking if the right columns exist and if they dont, attempt to make them. counters are set up with the aim to be used later to find most dominant pairings amongst the text. Calls full_dictionary_words_count()\n",
    "3. **full_dictionary_words_count()** function called. Creates empty dictionary that fills with pairings. Calls dict_word_collection() to count all of the pairings. Updated dictionary is called.\n",
    "4. Said fiilled dictionary with most dominat word pairings based on **limit** are passed to **show_word_cloud_via_dict()** function to create and display a word cloud.\n",
    "\n",
    "Process is called on all existing columns of bigrams/trigrams and quadgramns, just needs to be called on the dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Idea is that the text is already cleaned\n",
    "# Using bigrams and tri grams functions to apply to dataframe\n",
    "def extract_ngrams_2(text):\n",
    "    doc = word_tokenize(text)\n",
    "    ngrams = [str((doc[i], doc[i+1])) for i in range(len(doc)-1)]\n",
    "    return ngrams\n",
    "\n",
    "def extract_ngrams_3(text):\n",
    "    doc = word_tokenize(text)\n",
    "    ngrams = [str((doc[i], doc[i+1], doc[i+2])) for i in range(len(doc)-2)]\n",
    "    return ngrams\n",
    "\n",
    "def extract_ngrams_4(text):\n",
    "    doc = word_tokenize(text)\n",
    "    ngrams = [str((doc[i], doc[i+1], doc[i+2], doc[i+3])) for i in range(len(doc)-3)]\n",
    "    return ngrams\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Input: Obs\n",
    "    Output:\n",
    "'''\n",
    "def dict_word_collection(obs_list, obs_dict, limit):\n",
    "    for k,v in obs_list.items():\n",
    "        if v > limit:\n",
    "            obs_dict[k] = v\n",
    "    return obs_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Input:\n",
    "    Output:\n",
    "'''\n",
    "def full_dictionary_words_count(counter_bi, counter_tri, counter_quad):\n",
    "    # Create dictionary of unique full counts, used for visualisation - bigrams and tri words\n",
    "    obs_bi_list, obs_tri_list, obs_quad_list  = dict(counter_bi), dict(counter_tri), dict(counter_quad)\n",
    "\n",
    "    # Initialise a set of 3 empty dictionaries\n",
    "    obs_bi_dictionary, obs_tri_dictionary, obs_quad_dictionary = {}, {}, {}\n",
    "\n",
    "    # Collect dictionary total of occurences\n",
    "    bi_dict_collection = dict_word_collection(obs_bi_list, obs_bi_dictionary, limit=5)\n",
    "    tri_dict_collection = dict_word_collection(obs_tri_list, obs_tri_dictionary, limit=5)\n",
    "    quad_dict_collection = dict_word_collection(obs_quad_list, obs_quad_dictionary, limit=5)\n",
    "\n",
    "    # Return the collection\n",
    "    return bi_dict_collection, tri_dict_collection, quad_dict_collection\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Input: Dataframe with cleaned text.\n",
    "    Output: Word clouds of bi-grams, tri-grams and quad-grams and updated dataframe\n",
    "'''\n",
    "def collect_dict_bi_tri_quad(df):\n",
    "\n",
    "    # Initialise counters\n",
    "    obs_counts_ngrams2 = Counter()\n",
    "    obs_counts_ngrams3 = Counter()\n",
    "    obs_counts_ngrams4 = Counter()\n",
    "\n",
    "    try:\n",
    "        # Apply if possible\n",
    "        df['bigrams'].apply(obs_counts_ngrams2.update)\n",
    "        df['trigrams'].apply(obs_counts_ngrams3.update)\n",
    "        df['quadgrams'].apply(obs_counts_ngrams4.update)\n",
    "    except Exception as e1:\n",
    "        print(f\"cols in DF must include - [['bigrams','trigrams','quadgrams']]. \\n Attempting to create one for user. \\n Make sure text col is named observation_cleaned (does not need to be processed using NLP) \\n-> {e1}\")\n",
    "\n",
    "        # If not created, create if for the user\n",
    "        try:\n",
    "            df['bigrams'] = df['observation_cleaned'].apply(extract_ngrams_2)\n",
    "            df['trigrams'] = df['observation_cleaned'].apply(extract_ngrams_3)\n",
    "            df['quadgrams'] = df['observation_cleaned'].apply(extract_ngrams_4)\n",
    "\n",
    "        except Exception as e2:\n",
    "            print(f\"Attempted to create one. You sure the text field is called observation_cleaned?. \\n-> {e2.__context____} // {e2}\")\n",
    "\n",
    "        else:\n",
    "            # Apply\n",
    "            df['bigrams'].apply(obs_counts_ngrams2.update)\n",
    "            df['trigrams'].apply(obs_counts_ngrams3.update)\n",
    "            df['quadgrams'].apply(obs_counts_ngrams4.update)\n",
    "\n",
    "            return full_dictionary_words_count(obs_counts_ngrams2, obs_counts_ngrams3, obs_counts_ngrams4)\n",
    "\n",
    "    else:\n",
    "        return full_dictionary_words_count(obs_counts_ngrams2, obs_counts_ngrams3, obs_counts_ngrams4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Input:\n",
    "    Output:\n",
    "'''\n",
    "def show_word_cloud_via_dict(dict_col):\n",
    "    # Word cloud for observations or bi-grams\n",
    "    wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(dict_col)\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Apply\n",
    "df['bigrams'] = df['observation_cleaned'].apply(extract_ngrams_2)\n",
    "df['trigrams'] = df['observation_cleaned'].apply(extract_ngrams_3)\n",
    "df['quadgrams'] = df['observation_cleaned'].apply(extract_ngrams_4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get dictionary of total occurances count of pairs of words\n",
    "bi_collection_dict, tri_collection_dict, quad_collection_dict = collect_dict_bi_tri_quad(df)\n",
    "\n",
    "# Show the word clouds\n",
    "print(\"----- OVERALL COLLECTION -----\")\n",
    "show_word_cloud_via_dict(bi_collection_dict)\n",
    "show_word_cloud_via_dict(tri_collection_dict)\n",
    "show_word_cloud_via_dict(quad_collection_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA - NLP: BIGRAMS/TRIGRAMS -> AVALANCHE/AREA OCCURANCE (05)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise conditions\n",
    "condition_yes = df['avalanche_Y/N'] == 'Yes'\n",
    "condition_no = df['avalanche_Y/N'] == 'No'\n",
    "\n",
    "# Get the df copies\n",
    "df_yes = df[condition_yes][['avalanche_Y/N', 'observation_cleaned_tokens', 'bigrams', 'trigrams', 'quadgrams']].copy()\n",
    "df_no = df[condition_no][['avalanche_Y/N', 'observation_cleaned_tokens', 'bigrams', 'trigrams', 'quadgrams']].copy()\n",
    "\n",
    "# Get dictionary of total occurances count of pairs of words\n",
    "bi_collection_y_dict, tri_collection_y_dict, quad_collection_y_dict = collect_dict_bi_tri_quad(df_yes)\n",
    "bi_collection_n_dict, tri_collection_n_dict, quad_collection_n_dict = collect_dict_bi_tri_quad(df_no)\n",
    "\n",
    "# Show the word clouds\n",
    "print(\"----- YES COLLECTION -----\")\n",
    "show_word_cloud_via_dict(bi_collection_y_dict)\n",
    "show_word_cloud_via_dict(tri_collection_y_dict)\n",
    "show_word_cloud_via_dict(quad_collection_y_dict)\n",
    "\n",
    "print(\"\\n----- NO COLLECTION -----\")\n",
    "show_word_cloud_via_dict(bi_collection_n_dict)\n",
    "show_word_cloud_via_dict(tri_collection_n_dict)\n",
    "show_word_cloud_via_dict(quad_collection_n_dict)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Delete occurances of bigrams and trigrams that are empty as that means there is nothing interesting to deal with\n",
    "df = df[df['trigrams'].notna()]\n",
    "df = df[df['bigrams'].notna()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORTANT**: interesting way of texturusing snow:\n",
    "\n",
    "Via the bigram and trigram dictionary word clouds that were made, im able to find most dominant terms and then categorise them slightly better. By using public snow observation sentances that were gathered from nwac.ac take most dominant combinations and categorise them into the following\n",
    "1. Strong\n",
    "2. Weak\n",
    "3. Wet\n",
    "4. Frozen\n",
    "5. FRESH\n",
    "6. STORM\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bi_collection_y_dict = dict(sorted(bi_collection_y_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "bi_collection_n_dict = dict(sorted(bi_collection_n_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "tri_collection_y_dict = dict(sorted(tri_collection_y_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "tri_collection_n_dict = dict(sorted(tri_collection_n_dict.items(), key=lambda item: item[1], reverse=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Input:\n",
    "    Output:\n",
    "'''\n",
    "\n",
    "## Make multi-labeled instead of just one.\n",
    "def texturise_snow_bi(observation_arr):\n",
    "    # For each bi/trigram in the observation, find a match and assign to columns accordingly?\n",
    "    bool_wet, bool_dry, bool_weak, bool_strong, bool_frozen, bool_storm = False, False, False, False, False, False\n",
    "    string_arr = []\n",
    "    for i in observation_arr:\n",
    "        # print(i)\n",
    "        i = str(i)\n",
    "        if i in wet_bi_arr and bool_wet == False and bool_dry == False:\n",
    "            print('Returning wet')\n",
    "            string_arr += ['Wet']\n",
    "            bool_wet = True\n",
    "        elif i in dry_bi_arr and bool_dry == False and bool_wet == False:\n",
    "            print('returning dry')\n",
    "            string_arr += ['Dry']\n",
    "            bool_dry = True\n",
    "        elif i in weak_bi_arr and bool_weak == False and bool_strong == False:\n",
    "            print('returning weak')\n",
    "            string_arr += ['Weak']\n",
    "            bool_weak = True\n",
    "            # return 'Weak'\n",
    "        elif i in strong_bi_arr and bool_strong == False and bool_weak == False:\n",
    "            print('returning strong')\n",
    "            string_arr += ['Strong']\n",
    "            bool_strong = True\n",
    "        elif i in hoar_bi_arr and bool_frozen == False:\n",
    "            print('returning frozen')\n",
    "            string_arr += ['Frozen']\n",
    "            bool_frozen = True\n",
    "            # return 'Frozen'\n",
    "        elif i in storm_bi_arr and bool_storm == False:\n",
    "            string_arr += ['Storm']\n",
    "            bool_storm = True\n",
    "\n",
    "\n",
    "    ## Apply set logic to make sure the combinations are thesame\n",
    "    if set(string_arr) == {'Wet', 'Strong'}:\n",
    "        return ['Wet', 'Strong']\n",
    "    elif set(string_arr) == {'Wet', 'Weak'}:\n",
    "        return ['Wet', 'Weak']\n",
    "    elif set(string_arr) == {'Wet', 'Storm'}:\n",
    "        return ['Wet', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Frozen'}:\n",
    "        return ['Wet', 'Frozen']\n",
    "    elif set(string_arr) == {'Wet', 'Strong', 'Storm'}:\n",
    "        return ['Wet', 'Strong', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Strong', 'Frozen'}:\n",
    "        return ['Wet', 'Strong', 'Frozen']\n",
    "    elif set(string_arr) == {'Wet', 'Strong', 'Frozen', 'Storm'}:\n",
    "        return ['Wet', 'Strong', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Weak', 'Storm'}:\n",
    "        return ['Wet', 'Weak', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Frozen', 'Storm'}:\n",
    "        return ['Wet', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Weak', 'Frozen'}:\n",
    "        return ['Wet', 'Weak', 'Frozen']\n",
    "    elif set(string_arr) == {'Wet', 'Weak', 'Storm', 'Frozen'}:\n",
    "        return ['Wet', 'Weak', 'Storm', 'Frozen']\n",
    "    elif set(string_arr) == {'Dry', 'Strong'}:\n",
    "        return ['Dry', 'Strong']\n",
    "    elif set(string_arr) == {'Dry', 'Weak'}:\n",
    "        return ['Dry', 'Weak']\n",
    "    elif set(string_arr) == {'Dry', 'Storm'}:\n",
    "        return ['Dry', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Hoar'}:\n",
    "        return ['Dry', 'Hoar']\n",
    "    elif set(string_arr) == {'Dry', 'Strong', 'Storm'}:\n",
    "        return ['Dry', 'Strong', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Strong', 'Frozen'}:\n",
    "        return ['Dry', 'Strong', 'Frozen']\n",
    "    elif set(string_arr) == {'Dry', 'Strong', 'Frozen', 'Storm'}:\n",
    "        return ['Dry', 'Strong', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Weak', 'Storm'}:\n",
    "        return ['Dry', 'Weak', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Weak', 'Frozen'}:\n",
    "        return ['Dry', 'Weak', 'Frozen']\n",
    "    elif set(string_arr) == {'Dry', 'Weak', 'Frozen', 'Storm'}:\n",
    "        return ['Dry', 'Weak', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Frozen', 'Storm'}:\n",
    "        return ['Dry', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Strong', 'Frozen'}:\n",
    "        return ['Strong', 'Frozen']\n",
    "    elif set(string_arr) == {'Strong', 'Storm'}:\n",
    "        return ['Strong', 'Storm']\n",
    "    elif set(string_arr) == {'Strong', 'Frozen', 'Storm'}:\n",
    "        return ['Strong', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Weak', 'Frozen'}:\n",
    "        return ['Weak', 'Frozen']\n",
    "    elif set(string_arr) == {'Weak', 'Frozen', 'Storm'}:\n",
    "        return ['Weak', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Weak', 'Storm'}:\n",
    "        return ['Weak', 'Storm']\n",
    "    elif set(string_arr) == {'Storm', 'Frozen'}:\n",
    "        return ['Storm', 'Frozen']\n",
    "\n",
    "    # if string_arr == []:\n",
    "    #     return 'Fresh'\n",
    "    return string_arr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def texturise_snow(observation_arr):\n",
    "    # For each bi/trigram in the observation, find a match and assign to columns accordingly?\n",
    "    bool_wet, bool_dry, bool_weak, bool_strong, bool_frozen, bool_storm = False, False, False, False, False, False\n",
    "    string_arr = []\n",
    "    for i in observation_arr:\n",
    "        # print(i)\n",
    "        i = str(i)\n",
    "        if i in wet_arr and bool_wet == False and bool_dry == False:\n",
    "            print('Returning wet')\n",
    "            string_arr += ['Wet']\n",
    "            bool_wet = True\n",
    "        elif i in dry_arr and bool_dry == False and bool_wet == False:\n",
    "            print('returning dry')\n",
    "            string_arr += ['Dry']\n",
    "            bool_dry = True\n",
    "        elif i in weak_arr and bool_weak == False and bool_strong == False:\n",
    "            print('returning weak')\n",
    "            string_arr += ['Weak']\n",
    "            bool_weak = True\n",
    "            # return 'Weak'\n",
    "        elif i in strong_arr and bool_strong == False and bool_weak == False:\n",
    "            print('returning strong')\n",
    "            string_arr += ['Strong']\n",
    "            bool_strong = True\n",
    "        elif i in hoar_arr and bool_frozen == False:\n",
    "            print('returning frozen')\n",
    "            string_arr += ['Frozen']\n",
    "            bool_frozen = True\n",
    "            # return 'Frozen'\n",
    "        elif i in storm_arr and bool_storm == False:\n",
    "            string_arr += ['Storm']\n",
    "            bool_storm = True\n",
    "\n",
    "    ## Apply set logic to make sure the combinations are thesame\n",
    "    if set(string_arr) == {'Wet', 'Strong'}:\n",
    "        return ['Wet', 'Strong']\n",
    "    elif set(string_arr) == {'Wet', 'Weak'}:\n",
    "        return ['Wet', 'Weak']\n",
    "    elif set(string_arr) == {'Wet', 'Storm'}:\n",
    "        return ['Wet', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Frozen'}:\n",
    "        return ['Wet', 'Frozen']\n",
    "    elif set(string_arr) == {'Wet', 'Strong', 'Storm'}:\n",
    "        return ['Wet', 'Strong', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Strong', 'Frozen'}:\n",
    "        return ['Wet', 'Strong', 'Frozen']\n",
    "    elif set(string_arr) == {'Wet', 'Strong', 'Frozen', 'Storm'}:\n",
    "        return ['Wet', 'Strong', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Weak', 'Storm'}:\n",
    "        return ['Wet', 'Weak', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Frozen', 'Storm'}:\n",
    "        return ['Wet', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Wet', 'Weak', 'Frozen'}:\n",
    "        return ['Wet', 'Weak', 'Frozen']\n",
    "    elif set(string_arr) == {'Wet', 'Weak', 'Storm', 'Frozen'}:\n",
    "        return ['Wet', 'Weak', 'Storm', 'Frozen']\n",
    "    elif set(string_arr) == {'Dry', 'Strong'}:\n",
    "        return ['Dry', 'Strong']\n",
    "    elif set(string_arr) == {'Dry', 'Weak'}:\n",
    "        return ['Dry', 'Weak']\n",
    "    elif set(string_arr) == {'Dry', 'Storm'}:\n",
    "        return ['Dry', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Hoar'}:\n",
    "        return ['Dry', 'Hoar']\n",
    "    elif set(string_arr) == {'Dry', 'Strong', 'Storm'}:\n",
    "        return ['Dry', 'Strong', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Strong', 'Frozen'}:\n",
    "        return ['Dry', 'Strong', 'Frozen']\n",
    "    elif set(string_arr) == {'Dry', 'Strong', 'Frozen', 'Storm'}:\n",
    "        return ['Dry', 'Strong', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Weak', 'Storm'}:\n",
    "        return ['Dry', 'Weak', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Weak', 'Frozen'}:\n",
    "        return ['Dry', 'Weak', 'Frozen']\n",
    "    elif set(string_arr) == {'Dry', 'Weak', 'Frozen', 'Storm'}:\n",
    "        return ['Dry', 'Weak', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Dry', 'Frozen', 'Storm'}:\n",
    "        return ['Dry', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Strong', 'Frozen'}:\n",
    "        return ['Strong', 'Frozen']\n",
    "    elif set(string_arr) == {'Strong', 'Storm'}:\n",
    "        return ['Strong', 'Storm']\n",
    "    elif set(string_arr) == {'Strong', 'Frozen', 'Storm'}:\n",
    "        return ['Strong', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Weak', 'Frozen'}:\n",
    "        return ['Weak', 'Frozen']\n",
    "    elif set(string_arr) == {'Weak', 'Frozen', 'Storm'}:\n",
    "        return ['Weak', 'Frozen', 'Storm']\n",
    "    elif set(string_arr) == {'Weak', 'Storm'}:\n",
    "        return ['Weak', 'Storm']\n",
    "    elif set(string_arr) == {'Storm', 'Frozen'}:\n",
    "        return ['Storm', 'Frozen']\n",
    "\n",
    "    return string_arr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Initialisation of categorical snow textures\n",
    "wet_bi_arr =[\n",
    "    \"('rain', 'crust')\",\"('wet', 'snow')\",\"('melt', 'forms')\",\"('loose', 'wet')\",\"('moist', 'snow')\",\"('small', 'wet')\",\"('wet', 'avalanches')\",\"('snow', 'wet')\",\"('snow', 'moist')\",\"('light', 'rain')\",\"('wet', 'slide')\" ,\"('thick', 'melt')\",\"('hard', 'melt')\",\"('wet', 'loose')\", \"('wet', 'slides')\",\"('wet', 'avalanches')\",\"('wet', 'slabs')\", \"('remained', 'wet')\", \"('wetter', 'snow')\"\n",
    "]\n",
    "dry_bi_arr = [\n",
    "    \"('dry', 'snow')\",\"('dry', 'loose')\",\"('soft', 'snow')\",\"('loose', 'dry')\",\"('cold', 'dry')\",\"('dry', 'powder')\",\"('dry', 'avalanches')\",\"('dry', 'powder')\", \"('warm', 'weather')\"\n",
    "]\n",
    "weak_bi_arr = [\n",
    "    \"('low', 'density')\",\"('weak', 'snow')\",\"('thin', 'crust')\",\"('weaker', 'snow')\",\"('loose', 'activity')\",\"('poorly', 'bonded')\",\"('loose', 'snow')\",\"('weak', 'layers')\", \"('weak', 'layer')\", \"('softer', 'surface')\", \"('sun', 'crust')\", \"('new', 'snow')\"\n",
    "]\n",
    "strong_bi_arr = [\n",
    "    \"('well', 'bonded')\",\"('density', 'snow')\",\"('hard', 'crust')\",\"('firm', 'crust')\",\"('thick', 'crust')\",\"('bonded', 'crust')\",\"('bonding', 'well')\",\"('snow', 'hard')\", \"('snow', 'heavy')\", \"('stable', 'snow')\"\n",
    "]\n",
    "## Seperate\n",
    "storm_bi_arr =[\n",
    "    \"('recent', 'storm')\",\"('new', 'storm')\",\"('mid', 'storm')\",\"('storm', 'layer')\"\n",
    "]\n",
    "# aka frozen\n",
    "hoar_bi_arr = [\n",
    "    \"('melt', 'freeze')\", \"('freeze', 'crust')\",\"('freezing', 'rain')\",\"('ice', 'crust')\",\"('ice', 'layer')\",\"('hoar', 'layer')\",\"('hoar', 'observed')\",\"('hoar', 'near')\",\"('hoar', 'frost')\", \"('hoary', 'surface')\", \"('hoar', 'surface')\", \"('surface', 'hoar')\"\n",
    "]\n",
    "\n",
    "# Single words for the nans after bigram check\n",
    "wet_arr = ['wet']\n",
    "dry_arr = ['dry', 'powder']\n",
    "weak_arr = ['weak']\n",
    "strong_arr = ['strong', 'durable', 'firm', 'consolidated', 'stable']\n",
    "storm_arr = ['storm']\n",
    "hoar_arr = ['frozen', 'cold']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Save to a column per each bigram/trigram extraction**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['snow_condition_per_bigram'] = df['bigrams'].apply(texturise_snow_bi)\n",
    "df['snow_condition_per_bigram'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['snow_condition_per_word'] = df['observation_cleaned_tokens'].apply(texturise_snow)\n",
    "df['snow_condition_per_word'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "def make_na(x):\n",
    "    if [] == x:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def make_fresh(x):\n",
    "    if [] == x:\n",
    "        return ['Fresh']\n",
    "    else:\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**FILL NEW NA FROM OBSERVATION EXTRACTIONS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Some came out as empty so setting them as nan\n",
    "df['snow_condition_per_bigram'] = df['snow_condition_per_bigram'].apply(make_na)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## SAVE IT! - check point sort of deal\n",
    "df.to_csv('avalanche_types.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['snow_condition_per_bigram'].fillna(df['snow_condition_per_word'], inplace=True)\n",
    "df['snow_condition_per_bigram'].value_counts(normalize=True)\n",
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make empty == null again and take out the ones that are not null\n",
    "df['snow_condition_per_bigram'] = df['snow_condition_per_bigram'].apply(make_na)\n",
    "df = df[df['snow_condition_per_bigram'].notna()]\n",
    "df.to_csv('avalanche_types.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['snow_condition_per_bigram'] = df['snow_condition_per_bigram'].apply(make_fresh)\n",
    "df['snow_condition_per_bigram'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('avalanche_type_fresh.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
