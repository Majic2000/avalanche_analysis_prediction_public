{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## General data processing and visualisation use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import glob\n",
    "import plotly.express as px\n",
    "\n",
    "## For webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "## Machine learning / Deep learning classification models\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "## XGBoost as extra\n",
    "import xgboost as xgb\n",
    "\n",
    "## Set the pandas display option set to max_columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "## Natural language processing\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get correct dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "df_n_fresh = pd.read_csv('df_n_fresh.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## function to check the inputted model type\n",
    "## raises value error.\n",
    "def check_model_type(model_type):\n",
    "    # Check valid model type\n",
    "    valid_models = [\"DecisionTree\", \"RandomForest\", \"LogisticRegression\", \"XGBoost\", \"ExtraTrees\", \"SVM\"]\n",
    "    if model_type not in valid_models:\n",
    "        raise ValueError(\"Invalid model type - TRY AGAIN\\n - TRY -> [DecisionTree, RandomForest, LogisticRegression, XGBoost, ExtraTrees, SVM]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Function to create any model\n",
    "## input: Takes in a X_train and a y_train\n",
    "## output: Model\n",
    "## SHOULD DEFINETLY IMPLEMENT PICKLE FOR A REAL PROJECT\n",
    "## MEANS WE WOULDN'T HAVE TO RETURN IT EACH TIME\n",
    "def train_model(X_train, y_train, model_type):\n",
    "\n",
    "    svm_linear_bool = False\n",
    "    svm_poly_bool = False\n",
    "\n",
    "    # Check input data and target shapes\n",
    "    if X_train.shape[0] != y_train.shape[0]:\n",
    "        raise ValueError(\"Shapes do not match! - TRY AGAIN\")\n",
    "\n",
    "    check_model_type(model_type)\n",
    "\n",
    "    if model_type == 'DecisionTree':\n",
    "        model = DecisionTreeClassifier(random_state=10)\n",
    "        param_grid = {'max_depth': [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}\n",
    "\n",
    "    elif model_type == 'ExtraTrees':\n",
    "        model = ExtraTreesClassifier()\n",
    "        param_grid = {\n",
    "            'n_estimators': [5,10,20,40,60,80,100,120, 150,175, 200],\n",
    "            'max_depth': [1, 2, 3, 4, 5, 6, 7,8,9,10,11,12,13,14,15,16]}\n",
    "\n",
    "    elif model_type == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=10, max_iter=300)\n",
    "        param_grid = {'C': [0.1, 0.5, 1, 5, 10, 25, 50, 75, 100]}\n",
    "\n",
    "    elif model_type == 'RandomForest':\n",
    "        model = RandomForestClassifier(random_state=10)\n",
    "        param_grid = {'n_estimators':[5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,100,125,150,200],\n",
    "                      'max_depth':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,20,32,64]}\n",
    "\n",
    "    elif model_type == 'XGBoost':\n",
    "        model = xgb.XGBClassifier()\n",
    "        param_grid = {'n_estimators': [25,50,100,150,200],\n",
    "                      'max_depth': [2,4,8,16,32],\n",
    "                      'learning_rate': [0.01, 0.05, 0.07, 0.1, 0.02, 0.03, 0.06, 0.08, 0.15]}\n",
    "    elif model_type == 'SVM':\n",
    "        model = SVC()\n",
    "        param_grid = {\n",
    "            'probability': [True],\n",
    "            'C': [0.01, 0.1, 0.5, 1, 5, 10,12,16,20],\n",
    "            'gamma': [50,40,30,20,10,5,2,1,0.1,0.01,0.001],\n",
    "            'kernel': ['sigmoid']\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type - please try again with the following options: \\n - LogisticRegression\\n - DecisionTree\\n - RandomForest\\n - SVM\")\n",
    "\n",
    "    ## Initialise and declare GridsearchCV and fit accordingly\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=10)\n",
    "    grid_search.fit(X_train, y_train.values.ravel())                    ############################\n",
    "    print(f'BEST PARAMS {grid_search.best_params_}\\n')\n",
    "\n",
    "    ## Assign to model_output\n",
    "    model_output = grid_search.best_estimator_\n",
    "\n",
    "    if model_type == 'LogisticRegression':\n",
    "        print(f'feature importance: \\n {grid_search.best_estimator_.coef_[0]}')\n",
    "        print(f\"mean : {np.mean(abs(cross_val_score(model, X_train, y_train, cv=10, scoring='f1_macro')))}\")\n",
    "        print(f\"std : {np.std(abs(cross_val_score(model, X_train, y_train, cv=10, scoring='f1_macro')))}\")\n",
    "        return model_output, model_output.feature_importances_\n",
    "    elif model_type == 'SVM':\n",
    "        # print(f'w: \\n {grid_search.best_estimator_.coef_}')\n",
    "        print(print('b = ', grid_search.best_estimator_.intercept_))\n",
    "        print('Indices of support vectors = ', grid_search.best_estimator_.support_)\n",
    "        print('Support vectors = ', grid_search.best_estimator_.support_vectors_)\n",
    "        print('Number of support vectors for each class = ', grid_search.best_estimator_.n_support_)\n",
    "        print('Coefficients of the support vector in the decision function = ', np.abs(grid_search.best_estimator_.dual_coef_))\n",
    "        print(f\"mean : {np.mean(abs(cross_val_score(model, X_train, y_train, cv=10, scoring='f1_macro')))}\")\n",
    "        print(f\"std : {np.std(abs(cross_val_score(model, X_train, y_train, cv=10, scoring='f1_macro')))}\")\n",
    "        return model_output, None\n",
    "    else:\n",
    "        print(f'feature importance: \\n {grid_search.best_estimator_.feature_importances_}')\n",
    "        print(f\"mean : {np.mean(abs(cross_val_score(model, X_train, y_train, cv=10, scoring='f1_macro')))}\")\n",
    "        print(f\"std : {np.std(abs(cross_val_score(model, X_train, y_train, cv=10, scoring='f1_macro')))}\")\n",
    "        return model_output, model_output.feature_importances_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bit of a counter intuative name. test model basically runs for train as well. Its a bool param to indeicate if were running for train or test.\n",
    "## INPUT: X, y, model:model, model_type:str, train:bool, threshold_input:float\n",
    "## OUTPUT: accuracy, precision, recall, f1, confusion, proba, prediction\n",
    "def test_model(X, y, model, model_type, train, threshold_input=None):\n",
    "\n",
    "    if train:\n",
    "\n",
    "        threshold_return = 0\n",
    "        accuracy_output = 0\n",
    "        precision_output = 0\n",
    "        recall_output = 0\n",
    "        f1_output = 0\n",
    "        confusion_output = None\n",
    "        prediction = 0\n",
    "        proba = 0\n",
    "        data_arr =[]\n",
    "\n",
    "        for epoch in range(1,21):\n",
    "            # print(f'epoch: {epoch}')\n",
    "            threshold = epoch * 0.05\n",
    "            proba = model.predict_proba(X)\n",
    "            prediction = (model.predict_proba(X)[:,1] >= threshold).astype(bool)\n",
    "\n",
    "            if model_type == 'SVM':\n",
    "                ## Evaluate the model\n",
    "                accuracy = accuracy_score(y, prediction)\n",
    "                precision = precision_score(y, prediction, average='weighted', labels=np.unique(prediction))\n",
    "                recall = recall_score(y, prediction, average='weighted', labels=np.unique(prediction))\n",
    "                f1 = f1_score(y, prediction, average='weighted', labels=np.unique(prediction))\n",
    "                confusion = confusion_matrix(y, prediction)\n",
    "                data_arr += [[threshold, accuracy, precision, recall, f1]]\n",
    "            else:\n",
    "                ## Evaluate the model\n",
    "                accuracy = accuracy_score(y, prediction)\n",
    "                precision = precision_score(y, prediction)\n",
    "                recall = recall_score(y, prediction)\n",
    "                f1 = f1_score(y, prediction)\n",
    "                confusion = confusion_matrix(y, prediction)\n",
    "                data_arr += [[threshold, accuracy, precision, recall, f1]]\n",
    "\n",
    "            ## Check recall as thats the focus\n",
    "            ## Cant have recall or precision to be 1 as then either completely guessing 1 or 0.\n",
    "            if recall_output < recall < 0.85 and accuracy > accuracy_output:\n",
    "                recall_output = recall\n",
    "                threshold_return = threshold\n",
    "                accuracy_output = accuracy\n",
    "                precision_output = precision\n",
    "                f1_output = f1\n",
    "                confusion_output = confusion\n",
    "\n",
    "\n",
    "        return accuracy_output, precision_output, recall_output, f1_output, confusion_output, proba, prediction, threshold_return, data_arr\n",
    "    else:\n",
    "        proba = model.predict_proba(X)\n",
    "        prediction = (model.predict_proba(X)[:,1] >= threshold_input).astype(bool)\n",
    "\n",
    "        if model_type == 'SVM':\n",
    "            ## Evaluate the model\n",
    "            accuracy = accuracy_score(y, prediction)\n",
    "            precision = precision_score(y, prediction, average='weighted', labels=np.unique(prediction))\n",
    "            recall = recall_score(y, prediction, average='weighted', labels=np.unique(prediction))\n",
    "            f1 = f1_score(y, prediction, average='weighted', labels=np.unique(prediction))\n",
    "            confusion = confusion_matrix(y, prediction)\n",
    "        else:\n",
    "            ## Evaluate the model\n",
    "            accuracy = accuracy_score(y, prediction)\n",
    "            precision = precision_score(y, prediction)\n",
    "            recall = recall_score(y, prediction)\n",
    "            f1 = f1_score(y, prediction)\n",
    "            confusion = confusion_matrix(y, prediction)\n",
    "\n",
    "        ## Return\n",
    "        return accuracy, precision, recall, f1, confusion, proba, prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Runs and uses all other function created to measure and evaluate perofmance\n",
    "## works for XGBoost, SVM, Extra trees, Random Forest, Decision tree.\n",
    "## Want to add deep learning.\n",
    "def evaluate_note_performance(X, y, model_type, training_data_type):\n",
    "\n",
    "    # Check valid model type\n",
    "    check_model_type(model_type)\n",
    "\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    print(f\"-- TRAINING AND TESTING {model_type} --\")\n",
    "    model, fe_imp = train_model(X_train, y_train, model_type)\n",
    "    train = test_model(X_train, y_train, model, model_type, train=True)\n",
    "    confusion_type_train, thershold_input, data_arr_train = train[4], train[7], train[8]\n",
    "    # thershold_input = train[7]\n",
    "    # data_arr_train = train[8]\n",
    "    print(f'THRESHOLD FOR {model_type} IS {thershold_input}')\n",
    "    test = test_model(X_test, y_test, model, model_type, train=False, threshold_input=thershold_input)\n",
    "    confusion_type_test = test[4]\n",
    "    print(f'Accuracy train: {train[0]} | Accuracy test: {test[0]}')\n",
    "    print(f'Precision train: {train[1]} | Precision test: {test[1]}')\n",
    "    print(f'Recall train: {train[2]} | Recall test: {test[2]}')\n",
    "    print(f'F1 train: {train[3]} | F1 test: {test[3]}')\n",
    "    print(f'Confusion train: {train[4]} \\n Confusion test: {test[4]}')\n",
    "\n",
    "    # if (model_type == 'DecisionTree' and training_data_type == 'np'):\n",
    "    #     return \"DATA SHOWN \\n\"\n",
    "    # else:\n",
    "    #\n",
    "    #     X_train[f'prob_sol_{model_type}_{training_data_type}'] = [lst[1] for lst in train[5]]\n",
    "    #     X_test[f'prob_sol_{model_type}_{training_data_type}'] = [lst[1] for lst in test[5]]\n",
    "    #\n",
    "    #     test_df = pd.concat([X_train,X_test])\n",
    "    #     sol_temp = pd.merge(sol_temp, test_df[[f'prob_sol_{model_type}_{training_data_type}']], left_index=True, right_index=True)\n",
    "    #     sol_temp[f'accuracy_{model_type}_{training_data_type}'] = test[0]\n",
    "    #\n",
    "    #     return sol_temp\n",
    "\n",
    "    # For evaluation - DO I NEED TO RETURN MODEL AS WELL?\n",
    "    if model_type == 'SVM':\n",
    "        return data_arr_train, confusion_type_train, confusion_type_test, model, None\n",
    "    else:\n",
    "        return data_arr_train, confusion_type_train, confusion_type_test, model, fe_imp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Gets data and provides illustrations based of accuracy, precusion, recall and f1 '''\n",
    "def show_eval_performance(data, fe=None):\n",
    "    id_vis = []\n",
    "    accuracy_vis = []\n",
    "    precision_vis = []\n",
    "    recall_vis = []\n",
    "    f1_vis = []\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    for i in data:\n",
    "        id_vis += [i[0]]\n",
    "        accuracy_vis += [i[1]]\n",
    "        precision_vis += [i[2]]\n",
    "        recall_vis += [i[3]]\n",
    "        f1_vis += [i[4]]\n",
    "\n",
    "    data_vis_df = pd.DataFrame({\n",
    "        'id':id_vis,\n",
    "        'acc':accuracy_vis,\n",
    "        'prec':precision_vis,\n",
    "        'recall':recall_vis,\n",
    "        'f1':f1_vis,\n",
    "    })\n",
    "\n",
    "    # plot lines\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.plot(id_vis, accuracy_vis, label = \"accuracy\")\n",
    "    plt.plot(id_vis, precision_vis, label = \"precision\")\n",
    "    plt.plot(id_vis, recall_vis, label = \"recall\")\n",
    "    plt.plot(id_vis, f1_vis, label = \"f1 values\")\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if fe is not None:\n",
    "\n",
    "        small_df = pd.DataFrame({\n",
    "            'features': features_n_svm1,\n",
    "            'importance': fe\n",
    "        })\n",
    "\n",
    "        small_df = small_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(\n",
    "            data=small_df,\n",
    "            x='features',\n",
    "            y='importance'\n",
    "        )\n",
    "        plt.xlabel('feature')\n",
    "        plt.ylabel('importance')\n",
    "        plt.xticks(\n",
    "            rotation=45,\n",
    "            horizontalalignment = 'right',\n",
    "            fontweight = 'light',\n",
    "            fontsize = 'large'\n",
    "        )\n",
    "        plt.title('feature importance')\n",
    "        plt.show()\n",
    "\n",
    "    return data_vis_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilisation of created functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Without month\n",
    "features_n = ['instability', 'Dry', 'Frozen', 'Storm', 'Strong', 'Weak', 'Wet',\n",
    "              'East Central', 'East North', 'East South', 'Mt Hood', 'Olympics',\n",
    "              'Other', 'Snoqualmie Pass', 'Stevens Pass', 'West Central',\n",
    "              'West North', 'West South', 'year', 'NWAC Forecaster', 'NWAC Observer', 'Pro', 'Public']\n",
    "\n",
    "\n",
    "features_n_svm1 = ['instability', 'Dry', 'Frozen', 'Storm', 'Strong', 'Weak', 'Wet',\n",
    "                   'East Central', 'East North', 'East South', 'Mt Hood', 'Olympics',\n",
    "                   'Other', 'Snoqualmie Pass', 'Stevens Pass', 'West Central',\n",
    "                   'West North', 'West South', 'NWAC Forecaster', 'NWAC Observer', 'Pro', 'Public', '2020', '2021', '2022', '2023']\n",
    "\n",
    "# With month\n",
    "features_m = ['instability', 'Dry', 'Frozen', 'Storm', 'Strong', 'Weak', 'Wet',\n",
    "              'East Central', 'East North', 'East South', 'Mt Hood', 'Olympics',\n",
    "              'Other', 'Snoqualmie Pass', 'Stevens Pass', 'West Central',\n",
    "              'West North', 'West South', 'year', 'NWAC Forecaster', 'NWAC Observer', 'Pro', 'Public', 'Apr', 'Dec', 'Feb', 'Jan',\n",
    "              'Jul', 'Jun', 'Mar', 'May', 'Nov','Oct', 'Sep']\n",
    "target = ['avalanche_Y/N']\n",
    "\n",
    "print(len(features_m), len(target))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_n_fresh.dtypes\n",
    "df_n_fresh['year'] = df_n_fresh['year'].astype('int')\n",
    "df_n_fresh.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DECISION TREE TRAINING AND EVALUATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Works better without month\n",
    "X = df_n_fresh[features_n]\n",
    "y = df_n_fresh[target]\n",
    "\n",
    "data_eval_decision_tree, cm_train, cm_test, model, fe_importance = evaluate_note_performance(X, y, model_type='DecisionTree', training_data_type='np')\n",
    "\n",
    "print(\"--- DECISION TREE ---\")\n",
    "plt.figure(figsize=(15,10))\n",
    "tree.plot_tree(\n",
    "    model, feature_names=features_n, class_names=['Yes', 'No'], filled=True\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "cmap = plt.cm.tab20c\n",
    "lab = ['Yes', 'No']\n",
    "\n",
    "print(\"--- TRAIN CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_train)\n",
    "\n",
    "print(\"--- TEST CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_test)\n",
    "\n",
    "print(\"--- OVERALL TRAIN EVALUATION ---\")\n",
    "df_disp = show_eval_performance(data_eval_decision_tree, fe_importance)\n",
    "df_disp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RANDOM FOREST TRAIN AND EVALUATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_n_fresh[features_m]\n",
    "y = df_n_fresh[target]\n",
    "\n",
    "data_eval_random_forest, cm_train, cm_test, model, fe = evaluate_note_performance(X, y, model_type='RandomForest', training_data_type='np')\n",
    "\n",
    "print(\"--- TRAIN CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_train)\n",
    "\n",
    "print(\"--- TEST CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_test)\n",
    "\n",
    "print(\"--- OVERALL TRAIN EVALUATION ---\")\n",
    "df_disp = show_eval_performance(data_eval_random_forest, fe)\n",
    "df_disp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXTRA TREE TRAIN AND EVAL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_n_fresh[features_m]\n",
    "y = df_n_fresh[target]\n",
    "\n",
    "data_eval_extratrees, cm_train, cm_test, model, fe = evaluate_note_performance(X, y, model_type='ExtraTrees', training_data_type='np')\n",
    "\n",
    "print(\"--- TRAIN CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_train)\n",
    "\n",
    "print(\"--- TEST CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_test)\n",
    "\n",
    "print(\"--- OVERALL TRAIN EVALUATION ---\")\n",
    "df_disp = show_eval_performance(data_eval_extratrees,fe)\n",
    "df_disp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBOOST TRAIN AND EVAL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_n_fresh[features_n_svm1]\n",
    "y = df_n_fresh[target]\n",
    "\n",
    "data_eval_xgboost, cm_train, cm_test, model, fe = evaluate_note_performance(X, y, model_type='XGBoost', training_data_type='np')\n",
    "\n",
    "print(\"--- TRAIN CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_train)\n",
    "\n",
    "print(\"--- TEST CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot(cmap=cmap)\n",
    "plt.show()\n",
    "print(cm_test)\n",
    "\n",
    "print(\"--- OVERALL TRAIN EVALUATION ---\")\n",
    "df_disp = show_eval_performance(data_eval_xgboost, fe)\n",
    "df_disp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM TRAIN AND EVAL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Heatmap as an SVM works on correlation\n",
    "sns.heatmap(df_n_fresh.corr())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_n_fresh[features_n_svm1]\n",
    "y = df_n_fresh[target]\n",
    "\n",
    "data_eval_svm, cm_train, cm_test, model, fe = evaluate_note_performance(X, y, model_type='SVM', training_data_type='np')\n",
    "\n",
    "print(\"--- TRAIN CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot()\n",
    "plt.show()\n",
    "print(cm_train)\n",
    "\n",
    "print(\"--- TEST CONFUSION MATRIX ---\")\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['No', 'Yes'])\n",
    "disp_cm.plot()\n",
    "plt.show()\n",
    "print(cm_test)\n",
    "\n",
    "print(\"--- OVERALL TRAIN EVALUATION ---\")\n",
    "df_disp = show_eval_performance(data_eval_svm)\n",
    "df_disp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance measure with temparature data.\n",
    "\n",
    "I think the best way to go about this to show a proof of conecpt is to get the main data weather data per given area, these include but will limit the size of my data set:\n",
    "- Mt Hood\n",
    "- Olympics\n",
    "- Stevens Pass\n",
    "- Snoqualmie Pass\n",
    "Will need to also test it on the original subset of the dataset with no temperature to get a clear idea of if there is an increase in performance or not.\n",
    "\n",
    "The data is very unballanced, thus need to try on specific areas and see if that improves things.\n",
    "- furhter need to cut\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
