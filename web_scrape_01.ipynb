{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## General imports #1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    All needed imports for the cells to run accordingly.\n",
    "'''\n",
    "## General data processing and visualisation use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import glob\n",
    "import plotly.express as px\n",
    "\n",
    "## For webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "## Machine learning / Deep learning classification models\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "## XGBoost as extra\n",
    "import xgboost as xgb\n",
    "\n",
    "## Set the pandas display option set to max_columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "## Natural language processing\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Web scraping funcions prep #2\n",
    "\n",
    "Data gethered from: nwac.ac -> washington mountsins public observations\n",
    "\n",
    "**WEBSITE IS VERY SPECIFIC**: There appear to be a varierity of mash ups between containers when looking in the detailed overview of an avalanche observation. Will need to copy the href link from detailed observations and call that seperately."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Function - extra_info_gather()\n",
    "    input - {str:site_link, arr:date, arr:zone, arr:location, arr:recent_av, arr:cracking, arr:collapsing, arr:info_observation, arr:cloud_cover, arr:wind, arr:advanced_observation_comments}\n",
    "    output - arrays appended with data from the web address \"site_link\"\n",
    "\n",
    "    NOTE: all of the paramater variable are arrays that get appedned with data from nwac.us. Specific to the website do to its caothic structure of containers and divs. Havent YET found a way to scrape it without getting all data.\n",
    "'''\n",
    "\n",
    "## FUTURE - THIS NEEDS TO BE CHANGED TO USE ASSERT INSTEAD OF TRY/CATCH BLOCKS\n",
    "def extra_info_gather(site_link, date, zone, location, recent_av, cracking, collapsing, info_observation, cloud_cover, wind, advanced_observation_comments):\n",
    "    response = requests.get(site_link)\n",
    "    time.sleep(0.1)\n",
    "    # verify_request(response)\n",
    "\n",
    "    # String to append to in order\n",
    "    temp_str = ''\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all of the divs from the observation details section\n",
    "    ## All seems to have a class of col-xs-12\n",
    "    all_info_divs_row_1 = soup.findAll('div', class_='col-xs-12')\n",
    "\n",
    "    ## Field-values seem to describe conditions and snow texture, save all into one?\n",
    "    all_info_advanced = soup.find('div', class_='advanced-observations')    #.findAll('p', class_='field-value')\n",
    "\n",
    "    ## Check\n",
    "    if type(all_info_advanced) != type(None):\n",
    "        all_info_advanced = all_info_advanced.findAll('p', class_='field-value')\n",
    "\n",
    "    # Holds cloud cover, and wind\n",
    "    all_info_weather_summary = soup.find('div', class_='advanced-observations')\n",
    "    if type(all_info_weather_summary) != type(None):\n",
    "        all_info_weather_summary = all_info_weather_summary.findAll('div', class_='col-xs-4')\n",
    "\n",
    "    ## Initialise found for wanted features as false.\n",
    "    date_added, zone_region_added, location_added, recent_av_added, cracking_added, collapsing_added, obs_added = False, False, False, False, False, False, False\n",
    "\n",
    "    ## Need to compare each txt, if match to what i want then store, otherwise return nan?\n",
    "    for div_instance in all_info_divs_row_1:\n",
    "        ## First take care of the observation_date\n",
    "        try:\n",
    "            date_bool = str(div_instance.h5.text) == 'Observation Date:'\n",
    "            zone_region_bool = str(div_instance.h5.text) == 'Zone or Region:'\n",
    "            location_bool = str(div_instance.h5.text) == 'Location:'\n",
    "            recent_av_bool = str(div_instance.h5.text) == 'Recent Avalanches? '\n",
    "            cracking_bool = str(div_instance.h5.text) == 'Cracking? '\n",
    "            collapsing_bool = str(div_instance.h5.text) == 'Collapsing? '\n",
    "\n",
    "            observation_full = div_instance.text\n",
    "            observation = observation_full.replace(str(div_instance.h5.text), '').strip()\n",
    "\n",
    "            ## Check if true and concat with array, further set flag to True (1)\n",
    "            if date_bool:\n",
    "                date += [observation]\n",
    "                date_added = True\n",
    "\n",
    "            elif zone_region_bool:\n",
    "                zone += [observation]\n",
    "                zone_region_added = True\n",
    "\n",
    "            elif location_bool:\n",
    "                location += [observation]\n",
    "                location_added = True\n",
    "\n",
    "            elif recent_av_bool:\n",
    "                recent_av += [observation]\n",
    "                recent_av_added = True\n",
    "\n",
    "            elif cracking_bool:\n",
    "                cracking += [observation]\n",
    "                cracking_added = True\n",
    "\n",
    "            elif collapsing_bool:\n",
    "                collapsing += [observation]\n",
    "                collapsing_added = True\n",
    "\n",
    "        except Exception as e1:\n",
    "            ## Cant be a h5 therefore must be something else, try h4 as thats the universal for observations on the website...\n",
    "            # print(f'Error came up as h5 could not be found - {e}')\n",
    "\n",
    "            try:\n",
    "                obs_bool = str(div_instance.h4.text) == 'Observations'\n",
    "                if obs_bool == True:\n",
    "                    text = str(div_instance.p.text).strip()\n",
    "                    info_observation += [text]\n",
    "                    obs_added = True\n",
    "            except Exception as e2:\n",
    "                print(f'observation is missing? - {e2.__context__} followed by {e2.__cause__}')\n",
    "\n",
    "\n",
    "    ## Weather summery data if there\n",
    "    if not all_info_weather_summary:\n",
    "        # populate with nan\n",
    "        cloud_cover += [np.nan]\n",
    "        wind += [np.nan]\n",
    "    elif type(all_info_weather_summary) is None:\n",
    "        cloud_cover += [np.nan]\n",
    "        wind += [np.nan]\n",
    "    else:\n",
    "        for i in all_info_weather_summary:\n",
    "            string = \" \".join(i.text.split())\n",
    "            if i.h5.text == 'Cloud Cover:':\n",
    "                string = string.replace('Cloud Cover:', '')\n",
    "                cloud_cover += [string]\n",
    "            if i.h5.text == 'Wind:':\n",
    "                string = string.replace('Wind:', '')\n",
    "                wind += [string]\n",
    "\n",
    "\n",
    "    ## Comments from advanced\n",
    "    if not all_info_advanced:\n",
    "        ## No data found, have to populate with nan\n",
    "        advanced_observation_comments += [np.nan]\n",
    "    elif type(all_info_advanced) is None:\n",
    "        advanced_observation_comments += [np.nan]\n",
    "    else:\n",
    "        for i in all_info_advanced:\n",
    "            temp_str += str(i.text).strip()\n",
    "        advanced_observation_comments += [temp_str]\n",
    "\n",
    "    # Which has not been added?\n",
    "    # If one has not, populate it with nan/relevant to keep size consistancy.\n",
    "    if date_added == False:\n",
    "        print(\"date added was not found\")\n",
    "        date += [np.nan]\n",
    "    if zone_region_added == False:\n",
    "        print(\"zone added was not found\")\n",
    "        zone += [np.nan]\n",
    "    if location_added == False:\n",
    "        print(\"location added was not found\")\n",
    "        location += [np.nan]\n",
    "    if recent_av_added == False:\n",
    "        print(\"av added was not found\")\n",
    "        recent_av += ['No']\n",
    "    if cracking_added == False:\n",
    "        print(\"cracking added was not found\")\n",
    "        cracking += ['None Reported']\n",
    "    if collapsing_added == False:\n",
    "        print(\"collapsing added was not found\")\n",
    "        collapsing += ['None Reported']\n",
    "    if obs_added == False:\n",
    "        print(\"observation comments added was not found\")\n",
    "        info_observation += [np.nan]\n",
    "\n",
    "    ## return\n",
    "    return date, zone, location, recent_av, cracking, collapsing, info_observation, cloud_cover, wind, advanced_observation_comments\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    Function - links_extra()\n",
    "    Input - {webdriver:browser, arr:links, arr:links_arr}\n",
    "    Output - arr:links arr, filled array with all links to each instance of an avalanche\n",
    "\n",
    "    NOTE: uses webdriver to get data and click accordingly to get more data, then passed to beautiful soup for extraction, does this repeatedly for every entry of the page to gather url links to extra information about avalanches. again specific to nwac.us.\n",
    "'''\n",
    "def links_extra(browser, links, links_arr):\n",
    "    length = len(links)\n",
    "    print(length)\n",
    "\n",
    "    for i in range(0, length-1):\n",
    "        ## Want to do it up to 49 as index 50 is an instagram link.\n",
    "        links[i].click()\n",
    "        # time.sleep()\n",
    "        ## Extract the open in new tab link\n",
    "        ## pass through to soup?\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'html')\n",
    "\n",
    "        full_link = soup.find('a', class_='obs-open-new').get('href')\n",
    "        links_arr += [full_link]\n",
    "\n",
    "        quit_link = browser.find_element(By.CLASS_NAME, 'close')\n",
    "        quit_link.click()\n",
    "\n",
    "    return links_arr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Collect data finds all the needed data from website using beautiful soup and appends\n",
    "''' '''\n",
    "def base_collect_data(soup, dates, observer, region, location, avalanche, instability):\n",
    "    '''\n",
    "        already made soup passed in, other variables are array arguments to be appended with data\n",
    "        returns the appended data from souped website\n",
    "    '''\n",
    "\n",
    "    dates += [y.a.text for y in soup.findAll('td', class_='date')]\n",
    "    observer += [y.text for y in soup.findAll('td', class_='observer')]\n",
    "    region += [y.text for y in soup.findAll('td', class_='zone')]\n",
    "    location += [y.text for y in soup.findAll('td', class_='location')]\n",
    "    avalanche += [y.text for y in soup.findAll('td', class_='avalanches')]\n",
    "    instability += [y.text for y in soup.findAll('td', class_='instability')]\n",
    "\n",
    "    return dates, observer, region, location, avalanche, instability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WEB SCRAPING EXTRACTION #3\n",
    "\n",
    "1. it seems that for the website a 500 error occurs if you give hand it too large of a range, seemed to work really well for 4 years of data at a time absolute max. Anything above that will return error 500."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    " This cell by means of using WebDriver can be upgraded to fit a verierity of inputted seasons\n",
    " Future improvements or if I have enough time left...\n",
    " Cell is very spcific for nwac as its structure on getting data via next buttons works slightly differently.\n",
    " Use of all previous created functions, this is the main cell that runs the web scrape.\n",
    "\n",
    " IMPORTANT NOTES:\n",
    "    - If wanting to scrape the nwac.us website given observations, you have a 45 second window after the window opens using where this cell sleeps using time, reason for that is so then one can input a range of dates per wanted data in the web driver instance, after 45 seconds the cell will start scraping the entire section and its pages accordingly.\n",
    "    - The website gets a request error 500 when exceeding a certain range of extracted dates, if wanting to extract everything:\n",
    "        - 1st extract from 1st JAN 2020 till 30th NOV 2022\n",
    "        - 2nd extract from 1st DEC 2022 till CURRENT (if you put a date above current, should still work)\n",
    "        - Thus needs to be run twice due to the server error that occurs when attempting to extract the full range.\n",
    "'''\n",
    "\n",
    "# Initialise arrays to store the data\n",
    "dates = []\n",
    "observer = []\n",
    "region = []\n",
    "location = []\n",
    "avalanche = []\n",
    "instability = []\n",
    "links = []\n",
    "\n",
    "# Initialsie extra arrays for the extra data insigts of each avalanche instance\n",
    "date_arr = []\n",
    "zone_arr = []\n",
    "location_arr = []\n",
    "recent_av_arr = []\n",
    "cracking_arr = []\n",
    "collapsing_arr = []\n",
    "info_observation_arr = []\n",
    "image_links_arr = []\n",
    "cloud_cover_arr = []\n",
    "wind_arr = []\n",
    "advanced_observation_comments_arr = []\n",
    "\n",
    "\n",
    "# Initialise the browser\n",
    "browser = webdriver.Chrome()\n",
    "site = 'https://nwac.us/observations/#/obs'\n",
    "browser.get(site)\n",
    "# Give it time for user to pick range within chrome instance.\n",
    "time.sleep(45)\n",
    "\n",
    "# Find all button indexes for pages\n",
    "selects = browser.find_elements(By.CLASS_NAME, 'paginate_button')\n",
    "\n",
    "# Minus 1 due to the last increment not changing the html layout, called twice\n",
    "len_run = int(selects[6].text) - 1\n",
    "# Layout of index pages changes on click 5\n",
    "first_change_limit = 5\n",
    "\n",
    "################################################\n",
    "# NOTE BELOW CAN BE OPTIMSED, 3 COPIES OF THESAME CODE CAN BE MADE INTO ONE FUNCTION\n",
    "# THINK ABOUT IF TIME ALLOWS.\n",
    "################################################\n",
    "\n",
    "\n",
    "# For loop to go through the website, dependent upon seasonal settings within the browser\n",
    "for i in range(0, len_run):\n",
    "\n",
    "    # Find all the buttons that change sub pages within the website\n",
    "    selects = browser.find_elements(By.CLASS_NAME, 'paginate_button')\n",
    "\n",
    "    # First 4 button clicks for next page are thesame\n",
    "    if i < first_change_limit:\n",
    "        print(f'--- CURRENT PAGE BEING SCRAPED {selects[i].text}')\n",
    "        selects[i].click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting extra links for extra info\n",
    "        extra_info = browser.find_elements(By.CLASS_NAME, 'strong')\n",
    "        links = links_extra(browser, extra_info, links)\n",
    "\n",
    "        # Get the source of the page and soup it, append arrays of data\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'html')\n",
    "        dates, observer, region, location, avalanche, instability = base_collect_data(\n",
    "            soup, dates, observer, region, location, avalanche, instability\n",
    "        )\n",
    "\n",
    "    # Layout changes after 5th click\n",
    "    elif i >= first_change_limit and i<=len_run-2:\n",
    "        print(f'--- CURRENT PAGE BEING SCRAPED {selects[4].text}')\n",
    "        selects[4].click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting extra links for extra info\n",
    "        extra_info = browser.find_elements(By.CLASS_NAME, 'strong')\n",
    "        links = links_extra(browser, extra_info, links)\n",
    "\n",
    "        # Get the source of the page and soup it, append arrays of data\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'html')\n",
    "        dates, observer, region, location, avalanche, instability = base_collect_data(\n",
    "            soup, dates, observer, region, location, avalanche, instability\n",
    "        )\n",
    "\n",
    "    # Last two clicks as layout does not update with the last 2 entires.\n",
    "    else:\n",
    "        try:\n",
    "            print(f'--- CURRENT PAGE BEING SCRAPED {selects[5].text}')\n",
    "            # Fetch the last 2 -> index 5 and 6\n",
    "            selects[5].click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Getting extra links for extra info\n",
    "            extra_info = browser.find_elements(By.CLASS_NAME, 'strong')\n",
    "            links = links_extra(browser, extra_info, links)\n",
    "\n",
    "            # Get the source of the page and soup it, append arrays of data\n",
    "            html = browser.page_source\n",
    "            soup = BeautifulSoup(html, 'html')\n",
    "            dates, observer, region, location, avalanche, instability = base_collect_data(\n",
    "                soup, dates, observer, region, location, avalanche, instability\n",
    "            )\n",
    "\n",
    "            selects = browser.find_elements(By.CLASS_NAME, 'paginate_button')\n",
    "            print(f'--- CURRENT PAGE BEING SCRAPED {selects[6].text}')\n",
    "            selects[6].click()\n",
    "\n",
    "            # Getting extra links for extra info\n",
    "            extra_info = browser.find_elements(By.CLASS_NAME, 'strong')\n",
    "            links = links_extra(browser, extra_info, links)\n",
    "\n",
    "            # Get the source of the page and soup it, append arrays of data\n",
    "            html = browser.page_source\n",
    "            soup = BeautifulSoup(html, 'html')\n",
    "            dates, observer, region, location, avalanche, instability = base_collect_data(\n",
    "                soup, dates, observer, region, location, avalanche, instability\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Exception will be raised\n",
    "            print(f'Raised exception - {e}')\n",
    "\n",
    "        print(\"--- EOW - End of web index - FINISHED ---\")\n",
    "\n",
    "## Check\n",
    "for site_link in links:\n",
    "    print(f\"SCRAPING EXTRA DETAILS - {site_link}\")\n",
    "\n",
    "    # Will update the extra details\n",
    "    date_arr, zone_arr, location_arr, recent_av_arr, cracking_arr, collapsing_arr, info_observation_arr, cloud_cover_arr, wind_arr, advanced_observation_comments_arr = extra_info_gather(site_link, date_arr, zone_arr, location_arr, recent_av_arr, cracking_arr, collapsing_arr, info_observation_arr, cloud_cover_arr, wind_arr, advanced_observation_comments_arr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# With collected data, import into pandas dataframe.\n",
    "df = pd.DataFrame({\n",
    "    'dates': dates,\n",
    "    'observer' : observer,\n",
    "    'region' : region,\n",
    "    'zone': zone_arr,\n",
    "    'location' : location,\n",
    "    'avalanche_Y/N' : avalanche,\n",
    "    'instability' : instability,\n",
    "    'links' : links,\n",
    "    'location_ele': location_arr,\n",
    "    'cracking': cracking_arr,\n",
    "    'collapsing': collapsing_arr,\n",
    "    'info_observation': info_observation_arr,\n",
    "    'advanced_observations': advanced_observation_comments_arr\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Usually save, however already been done so commented out\n",
    "# Remember, would need to be run twice on different ranges to get full data due to 500 web address error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save\n",
    "df.to_csv('avalanche_occurance_1/2.csv') # Change to whatever name you want"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Remember that the script needs to be run at least 2x as too large of a range in finding observations at nwac.ac fails to error code 500** -> Will need to concat the data together to get one complete dataframe.\n",
    "\n",
    "**IN MY CASE:** They have been saved as **\"avalanche_occurance_1.csv\"** and **\"avalanche_occurance_2.csv\"**\n",
    "\n",
    "**MORE IN SECOND FILE**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
