{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## General data processing and visualisation use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import glob\n",
    "import plotly.express as px\n",
    "\n",
    "## For webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "## Machine learning / Deep learning classification models\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "## XGBoost as extra\n",
    "import xgboost as xgb\n",
    "\n",
    "## Set the pandas display option set to max_columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "## Natural language processing\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_n_fresh = pd.read_csv('avalanche_types.csv')\n",
    "df_n_fresh.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_n_fresh.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Function to calculate the amoung of NULLs in a dataframe per its given columns\n",
    "--- INPUT: pandas dataframe\n",
    "--- OUTPUT: pandas dataframe\n",
    "'''\n",
    "\n",
    "def calc_null(dataframe):\n",
    "\n",
    "    # Initialise a new df\n",
    "    missing_df = pd.DataFrame(df.isna().sum(), columns=['missing_vals'])\n",
    "    missing_df['percentage'] = 0.0\n",
    "\n",
    "    # Initialise math and loop var\n",
    "    total_count = len(dataframe)\n",
    "\n",
    "    # Assign percentage\n",
    "    missing_df['percentage'] = round(missing_df['missing_vals'] / total_count * 100, 3)\n",
    "\n",
    "    # Sort accordingly\n",
    "    missing_df = missing_df.sort_values('percentage', ascending=False)\n",
    "\n",
    "    return missing_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def year_extraction(x):\n",
    "    x = x.split(' ')\n",
    "    return x[3]\n",
    "\n",
    "def month_extraction(x):\n",
    "    x = x.split(' ')\n",
    "    return x[1]\n",
    "\n",
    "df_n_fresh['year'] = df_n_fresh['dates'].apply(year_extraction)\n",
    "df_n_fresh['month'] = df_n_fresh['dates'].apply(month_extraction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA (01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Shows the missing percentage df '''\n",
    "\n",
    "# Use previously created function to check amount of missing values and %\n",
    "df_missing_original = calc_null(df)\n",
    "\n",
    "# Reset the index\n",
    "df_missing_original_disp = df_missing_original.reset_index()\n",
    "\n",
    "# Initialise the figure and run seaborn for visualisations.\n",
    "plt.figure(figsize = (6, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data = df_missing_original_disp,\n",
    "    x = 'index',\n",
    "    y = 'percentage'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment='right',\n",
    "    fontweight ='light',\n",
    "    fontsize ='large'\n",
    ")\n",
    "plt.title('percentage of nulls per column')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Unique display of avalanches per given feature '''\n",
    "\n",
    "u_df = df.describe().loc['unique'].copy()\n",
    "temp_df = pd.DataFrame({'unique_count':u_df})\n",
    "temp_df = temp_df.sort_values('unique_count', ascending=False)\n",
    "values = temp_df.unique_count\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.barplot(\n",
    "    data=temp_df,\n",
    "    x=temp_df.index,\n",
    "    y='unique_count'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment = 'right',\n",
    "    fontweight = 'light',\n",
    "    fontsize = 'large'\n",
    ")\n",
    "plt.title('unique counts per column')\n",
    "# plt.savefig('Y/N_avalanches_region.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df_n_fresh.groupby('region')[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" }).sort_values(['count'], ascending=False)\n",
    "\n",
    "# Initialise figure\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.barplot(\n",
    "    data=disp_df,\n",
    "    x='region',\n",
    "    y='count',\n",
    "    hue='avalanche_Y/N'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment = 'right',\n",
    "    fontweight = 'light',\n",
    "    fontsize = 'large'\n",
    ")\n",
    "plt.title('Number of occurances of avalanches per region')\n",
    "plt.xlabel('REGION', fontsize=\"15\")\n",
    "plt.ylabel('TOTAL', fontsize=\"15\")\n",
    "plt.ylabel()\n",
    "# plt.savefig('Y/N_avalanches_region.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df_n_fresh.groupby('year')[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" }).sort_values(['count'], ascending=False)\n",
    "\n",
    "# Initialise figure\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.barplot(\n",
    "    data=disp_df,\n",
    "    x='year',\n",
    "    y='count',\n",
    "    hue='avalanche_Y/N'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment = 'right',\n",
    "    fontweight = 'light',\n",
    "    fontsize = 'large'\n",
    ")\n",
    "plt.ylabel('TOTAL', fontsize=\"15\")\n",
    "plt.xlabel('YEAR', fontsize=\"15\")\n",
    "plt.title('Number of occurances of avalanches per year', fontsize=\"15\")\n",
    "# plt.savefig('Y/N_avalanches_region.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df_n_fresh.groupby(['region', 'year'])[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" })\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[3]: \"count2\" })\n",
    "\n",
    "condition = disp_df['count'] == 'Yes'\n",
    "test = disp_df[condition].copy()\n",
    "\n",
    "# disp_df\n",
    "figure = plt.figure(figsize=(5,5))\n",
    "sns.catplot(x='region', y='count2', hue='year', data=test, kind='bar', height=8,aspect=2)\n",
    "plt.xlabel('REGION', fontsize=17)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel('ACTIVE TOTAL', fontsize=17)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('avalanche ACTIVE occurances per region per year', fontsize=\"20\")\n",
    "plt.legend(fontsize=\"15\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df_n_fresh.groupby('month')[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" })\n",
    "# disp_df = disp_df.rename(columns={ disp_df.columns[3]: \"count2\" })\n",
    "\n",
    "# condition = disp_df['count'] == 'Yes'\n",
    "# test = disp_df[condition].copy()\n",
    "# disp_df\n",
    "\n",
    "# disp_df\n",
    "figure = plt.figure(figsize=(5,5))\n",
    "sns.catplot(x='month', y='count', hue='avalanche_Y/N', data=disp_df, kind='bar', height=8,aspect=2)\n",
    "plt.xlabel('MONTH', fontsize=17)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel('TOTAL', fontsize=17)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('avalanche occurance per month', fontsize=\"20\")\n",
    "plt.legend(fontsize=\"15\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df_n_fresh.groupby('snow_condition_per_bigram')[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" }).sort_values(['count'], ascending=False)\n",
    "\n",
    "# Initialise figure\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.barplot(\n",
    "    data=disp_df,\n",
    "    x='snow_condition_per_bigram',\n",
    "    y='count',\n",
    "    hue='avalanche_Y/N'\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    horizontalalignment = 'right',\n",
    "    fontweight = 'light',\n",
    "    fontsize = 'large'\n",
    ")\n",
    "plt.ylabel('TOTAL', fontsize=\"12\")\n",
    "plt.xlabel('SNOW TEXTURE CONDITION', fontsize=\"12\")\n",
    "plt.title('Number of observational occurances given snow texture')\n",
    "# plt.savefig('Y/N_avalanches_region.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## YES\n",
    "\n",
    "# Avalanche_Y/N will not have any nulls as been web scraped.\n",
    "disp_df = df_n_fresh.groupby(['region', 'snow_condition_per_bigram'])[['avalanche_Y/N']].value_counts().reset_index().copy()\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[2]: \"count\" })\n",
    "disp_df = disp_df.rename(columns={ disp_df.columns[3]: \"count2\" })\n",
    "\n",
    "condition = ((disp_df['count'] == 'Yes') & (disp_df['region'] == 'Snoqualmie Pass')) | ((disp_df['count'] == 'Yes') & (disp_df['region'] == 'Stevens Pass'))\n",
    "test = disp_df[condition].sort_values('count2').copy()\n",
    "\n",
    "# test\n",
    "figure = plt.figure(figsize=(5,2))\n",
    "sns.catplot(x='region', y='count2', hue='snow_condition_per_bigram', data=test, kind='bar', height=8,aspect=2)\n",
    "plt.xticks(fontsize=\"14\")\n",
    "plt.yticks(fontsize=\"14\")\n",
    "plt.title('Type of snow texture observations per regions', fontsize=\"16\")\n",
    "plt.ylabel('TOTAL', fontsize=\"14\")\n",
    "plt.xlabel('REGION', fontsize=\"14\")\n",
    "plt.legend(fontsize=\"12\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FEATURE ENGINEERING (03)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def one_to_one_yn(x):\n",
    "    if x == 'Yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Need to preprocess the snow condition per bigram\n",
    "def list_to_string(x):\n",
    "    x = x.strip('[')\n",
    "    x = x.strip(']')\n",
    "    x = x.replace(\"'\", \"\")\n",
    "    x = x.replace(' ', '')\n",
    "    x = x.split(',')\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def feature_eng_dummy_variables_1(df):\n",
    "\n",
    "    # Apply clean text\n",
    "    df['snow_condition'] = df['snow_condition_per_bigram'].apply(list_to_string)\n",
    "\n",
    "    # Get dummy variables aka OHE\n",
    "    one_hot_df = pd.get_dummies(df['snow_condition'].apply(pd.Series).stack()).sum(level=0)\n",
    "    df = df.join(one_hot_df)\n",
    "    one_hot_df = pd.get_dummies(df['zone'])\n",
    "    df = df.join(one_hot_df)\n",
    "    one_hot_df = pd.get_dummies(df['observer'])\n",
    "    df = df.join(one_hot_df)\n",
    "    one_hot_df = pd.get_dummies(df['month'])\n",
    "    df = df.join(one_hot_df)\n",
    "    one_hot_df = pd.get_dummies(df['year'])\n",
    "    df = df.join(one_hot_df)\n",
    "\n",
    "    # Delete any potencual copy columns\n",
    "    df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "\n",
    "    ## binary encoding for instability and avalancheYN\n",
    "    df['instability'] = df['instability'].apply(one_to_one_yn)\n",
    "    df['avalanche_Y/N'] = df['avalanche_Y/N'].apply(one_to_one_yn)\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Apply base feature engineering\n",
    "df_n_fresh = feature_eng_dummy_variables_1(df_n_fresh)\n",
    "df_n_fresh.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save\n",
    "df_n_fresh.to_csv('df_n_fresh.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
